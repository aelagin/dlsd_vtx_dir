{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex reconstruction in a large Directional Liquid Scintillator Detector\n",
    "Training data location: https://stash.osgconnect.net/public/elagin/vtx_dir_1el_norm/\n",
    "\n",
    "Testing data location: https://stash.osgconnect.net/public/elagin/test_vtx_dir_1el_norm/\n",
    "\n",
    "Data are .npz files with 3 numpy arrays. Where each row correspond to one event.\n",
    "\n",
    "\"x\" is a (1000,6300) array of \"independent variables\", photo-electron hits (theta, phi, and time). Hit coordinate encoding in each row is the folowing: \\\n",
    "theta = x[:,0::3] \\\n",
    "phi = x[:,1::3] \\\n",
    "time = x[:,2::3] \\\n",
    "All data is normalized as theta = theta/pi, phi = phi/(2pi), time = time/maxT (currently maxT=150). \\\n",
    "Each row in \"x\" is padded with -1 to make it a length of 6300 \n",
    "\n",
    "\"y_vtx\" is a (1000,3) array of \"dependent variable\", true x-,y-,z- coordinates of the event vertex to be reconstructed \\\n",
    "\"y_dir\" is a (1000,3) array of \"dependent variable\", true x-,y-,z- coordinates of the unit vector along the original direction of the electron\n",
    "\n",
    "See checking_inputs.ipynb and checking_inputs_norm.ipynb for more details on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape, Lambda, Input, Concatenate, concatenate\n",
    "from keras.layers import Conv1D, Conv2D, Conv2DTranspose, UpSampling2D, MaxPooling1D, MaxPooling2D, LocallyConnected2D, LocallyConnected1D\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Masking\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "maxT = 37\n",
    "maxLambda = 800\n",
    "PAD_VALUE = -1\n",
    "NEvts = 1000\n",
    "pi = 3.141592653589793\n",
    "NFiles = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom generator to load large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class MyGenerator(Sequence):\n",
    "    def __init__(self, x_dir, batch_size):\n",
    "        self.x_dir = x_dir\n",
    "        self.x = os.listdir(x_dir)\n",
    "        self.batch_size = batch_size\n",
    "        print('Generator init complete')\n",
    "        \n",
    "    def __len__(self):\n",
    "        nf = self.batch_size/(NEvts) # calculate number of files\n",
    "        return math.ceil(len(self.x) / nf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_data = np.load(os.path.join(self.x_dir, self.x[idx]))\n",
    "        Y = np.array(batch_data['y_vtx'])\n",
    "        return np.array(batch_data['x']), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator init complete\n"
     ]
    }
   ],
   "source": [
    "training_generator = MyGenerator('/data/Elagin/vtx_dir_1el_color_norm', NEvts)\n",
    "\n",
    "#testing_generator = MyGenerator('/data/Elagin/test_vtx_dir_1el_color_norm', NEvts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data that can fit into RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from x_dir, stop after loading fn_max files\n",
    "# Returns X, Y, Dir\n",
    "# X - independent variables\n",
    "# Y - dependend variables (x-, y-, z- of the vertex in this implementation)\n",
    "# Dir - direction vector (x-, y-, z- componenets), could be made dependent variable\n",
    "def load_test_data_to_memory(x_dir, fn_max=2000) :\n",
    "    X, Y, Dir = np.array([]), np.array([]), np.array([])\n",
    "    f_list = os.listdir(x_dir)\n",
    "    f_list.sort()\n",
    "    i=0\n",
    "    for x in f_list :\n",
    "        batch_data = np.load(os.path.join(x_dir, x))\n",
    "        \n",
    "        X = np.vstack([X, np.array(batch_data['x'])]) if X.size else np.array(batch_data['x'])\n",
    "        Y = np.vstack([Y, np.array(batch_data['y_vtx'])]) if Y.size else np.array(batch_data['y_vtx'])\n",
    "        Dir = np.vstack([Dir, np.array(batch_data['y_dir'])]) if Dir.size else np.array(batch_data['y_dir'])\n",
    "\n",
    "        i+=1\n",
    "        if i%10 == 0:\n",
    "            print(x)\n",
    "            print('i = ', i, '   ', X.shape, '   ', Y.shape, '   ', Dir.shape)\n",
    "        if i>=fn_max:\n",
    "            break\n",
    "    print(X.shape, '   ', Y.shape, '   ', Dir.shape)\n",
    "    return X, Y, Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_batch_18.npz\n",
      "i =  10     (10000, 8400)     (10000, 3)     (10000, 3)\n",
      "f_batch_27.npz\n",
      "i =  20     (20000, 8400)     (20000, 3)     (20000, 3)\n",
      "f_batch_36.npz\n",
      "i =  30     (30000, 8400)     (30000, 3)     (30000, 3)\n",
      "f_batch_45.npz\n",
      "i =  40     (40000, 8400)     (40000, 3)     (40000, 3)\n",
      "(49000, 8400)     (49000, 3)     (49000, 3)\n"
     ]
    }
   ],
   "source": [
    "testX, testY, trueDir = load_test_data_to_memory('/data/Elagin/test_vtx_dir_1el_color_norm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_batch_109.npz\n",
      "i =  10     (10000, 8400)     (10000, 3)     (10000, 3)\n",
      "f_batch_119.npz\n",
      "i =  20     (20000, 8400)     (20000, 3)     (20000, 3)\n",
      "f_batch_129.npz\n",
      "i =  30     (30000, 8400)     (30000, 3)     (30000, 3)\n",
      "f_batch_139.npz\n",
      "i =  40     (40000, 8400)     (40000, 3)     (40000, 3)\n",
      "f_batch_149.npz\n",
      "i =  50     (50000, 8400)     (50000, 3)     (50000, 3)\n",
      "f_batch_159.npz\n",
      "i =  60     (60000, 8400)     (60000, 3)     (60000, 3)\n",
      "f_batch_1608.npz\n",
      "i =  70     (70000, 8400)     (70000, 3)     (70000, 3)\n",
      "f_batch_1617.npz\n",
      "i =  80     (80000, 8400)     (80000, 3)     (80000, 3)\n",
      "f_batch_1626.npz\n",
      "i =  90     (90000, 8400)     (90000, 3)     (90000, 3)\n",
      "f_batch_1635.npz\n",
      "i =  100     (100000, 8400)     (100000, 3)     (100000, 3)\n",
      "f_batch_1644.npz\n",
      "i =  110     (110000, 8400)     (110000, 3)     (110000, 3)\n",
      "f_batch_1653.npz\n",
      "i =  120     (120000, 8400)     (120000, 3)     (120000, 3)\n",
      "f_batch_1662.npz\n",
      "i =  130     (130000, 8400)     (130000, 3)     (130000, 3)\n",
      "f_batch_1671.npz\n",
      "i =  140     (140000, 8400)     (140000, 3)     (140000, 3)\n",
      "f_batch_1680.npz\n",
      "i =  150     (150000, 8400)     (150000, 3)     (150000, 3)\n",
      "(150000, 8400)     (150000, 3)     (150000, 3)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY, trueDir_training = load_test_data_to_memory('/data/Elagin/vtx_dir_1el_color_norm/',150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining various activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_to_pmOne(x) :\n",
    "    return 2*K.sigmoid(x) - 1\n",
    "\n",
    "def mapping_to_pm50(x) :\n",
    "    return mapping_to_pmOne(x)*50\n",
    "\n",
    "def mapping_to_pm300(x) :\n",
    "    return mapping_to_pmOne(x)*300\n",
    "\n",
    "def mapping_to_0_50(x) :\n",
    "    return 50*K.sigmoid(x)\n",
    "\n",
    "\n",
    "def mapping_to_target_range_all( x, target_min=-0.02*pi, target_max=2.02*pi ) :\n",
    "    x02 = K.tanh(x) + 1 # x in range(0,2)\n",
    "    scale = ( target_max-target_min )/2.\n",
    "    return  x02 * scale + target_min\n",
    "\n",
    "def mapping_to_target_range_2pi( x, target_min=-0.00*pi, target_max=2.00*pi ) :\n",
    "    x02 = K.tanh(x) + 1 # x in range(0,2)\n",
    "    scale = ( target_max-target_min )/2.\n",
    "    return  x02 * scale + target_min\n",
    "    #return K.sigmoid(x)*target_max\n",
    "\n",
    "def mapping_to_target_range_pi( x, target_min=-0.00*pi, target_max=1.00*pi ) :\n",
    "    x02 = K.tanh(x) + 1 # x in range(0,2)\n",
    "    scale = ( target_max-target_min )/2.\n",
    "    return  x02 * scale + target_min\n",
    "    #return K.sigmoid(x)*target_max\n",
    "\n",
    "def mapping_to_target_range_phi( x, target_min=0, target_max=2*pi ) :\n",
    "    print('is x keras tensor:', K.is_keras_tensor(x))\n",
    "    print('x shape = ',K.int_shape(x))\n",
    "    print('x shape = ',K.shape(x))\n",
    "    #phi = K.placeholder(shape=(None,1))\n",
    "    phi = Input([1])\n",
    "    #theta = K.placeholder(shape=(None,1))\n",
    "    theta = Input([1])\n",
    "    #r = K.placeholder(shape=(None,1))\n",
    "    r = Input([1])\n",
    "    print('phi shape = ',K.int_shape(phi))\n",
    "    print('is phi keras tensor:', K.is_keras_tensor(phi))\n",
    "    phi = K.concatenate([phi, [K.tanh(x[:,1]) + 1]], axis=0) # x in range(0,2)\n",
    "    #phi = K.tanh(x[:,1]) + 1\n",
    "    theta = K.concatenate([theta, [x[:,0]]], axis=0)\n",
    "    #theta = x[:,0]\n",
    "    r = K.concatenate([r, [x[:,2]]], axis=0)\n",
    "    #r = x[:,2]\n",
    "    print('phi shape = ',K.int_shape(phi))\n",
    "    print('is phi keras tensor:', K.is_keras_tensor(phi))\n",
    "    print('theta shape = ',K.int_shape(theta))\n",
    "    print('r shape = ',K.int_shape(r))\n",
    "    scale = ( target_max-target_min )/2.\n",
    "    phi =  phi * scale + target_min\n",
    "    #phi = K.cast(phi, dtype='int32')\n",
    "    #phi = K.reshape(phi, (,1))\n",
    "    print('phi shape = ',K.int_shape(phi))\n",
    "    \n",
    "    res = K.concatenate([theta, phi, r], axis=1)\n",
    "    print('res shape = ',K.int_shape(res))\n",
    "    print('res shape = ',K.shape(res))\n",
    "    print('is res keras tensor:', K.is_keras_tensor(res))\n",
    "    x[:,1].assign(phi)\n",
    "    return x\n",
    "\n",
    "def mapping_to_target_range_phi2( x, target_min=0, target_max=2*pi ) :\n",
    "    scale = ( target_max-target_min )/2.\n",
    "    phi = K.placeholder(shape=(None,1))\n",
    "    #phi = Input([1])\n",
    "    #phi = (K.tanh(x[:,1]) + 1)*scale + target_min\n",
    "    phi = K.concatenate([phi, [(K.tanh(x[:,1]) + 1)*scale + target_min]], axis=0)\n",
    "    x[:,1].assign(phi)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    N=8400\n",
    "    lstm_seq = 400\n",
    "    lstm_steps = int(N/lstm_seq)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Reshape(input_shape=(N,), target_shape=(lstm_steps,lstm_seq)))\n",
    "    \n",
    "    model.add(Masking(mask_value=-1, input_shape=(lstm_steps, lstm_seq)))\n",
    "    \n",
    "    model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "#     model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "#     model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "#     model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "#     model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "#     model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "    model.add(LSTM(75, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(3, activation=mapping_to_pm300))\n",
    "#    model.add(Dense(3, activation=None))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network=createModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 21, 400)           0         \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 21, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 21, 75)            142800    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 75)                45300     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               19456     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 274,119\n",
      "Trainable params: 274,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "150/150 [==============================] - 19s 127ms/step - loss: 5864.3276 - val_loss: 3336.1558\n",
      "Epoch 2/50\n",
      "150/150 [==============================] - 16s 107ms/step - loss: 2491.8989 - val_loss: 1791.3013\n",
      "Epoch 3/50\n",
      "150/150 [==============================] - 16s 109ms/step - loss: 1826.0656 - val_loss: 1998.8441\n",
      "Epoch 4/50\n",
      "150/150 [==============================] - 16s 110ms/step - loss: 1323.8636 - val_loss: 1243.7802\n",
      "Epoch 5/50\n",
      "150/150 [==============================] - 17s 111ms/step - loss: 1165.0938 - val_loss: 866.8926\n",
      "Epoch 6/50\n",
      "150/150 [==============================] - 15s 102ms/step - loss: 928.5143 - val_loss: 735.5507\n",
      "Epoch 7/50\n",
      "150/150 [==============================] - 15s 103ms/step - loss: 1012.3073 - val_loss: 868.1401\n",
      "Epoch 8/50\n",
      "150/150 [==============================] - 14s 95ms/step - loss: 861.2568 - val_loss: 1314.7109\n",
      "Epoch 9/50\n",
      "150/150 [==============================] - 16s 104ms/step - loss: 739.7809 - val_loss: 632.4205\n",
      "Epoch 10/50\n",
      "150/150 [==============================] - 16s 104ms/step - loss: 624.3921 - val_loss: 512.1712\n",
      "Epoch 11/50\n",
      "150/150 [==============================] - 15s 103ms/step - loss: 669.9361 - val_loss: 1849.5977\n",
      "Epoch 12/50\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 625.5826 - val_loss: 857.3547\n",
      "Epoch 13/50\n",
      "150/150 [==============================] - 16s 109ms/step - loss: 417.3042 - val_loss: 285.6367\n",
      "Epoch 14/50\n",
      "150/150 [==============================] - 15s 100ms/step - loss: 231.2828 - val_loss: 337.2224\n",
      "Epoch 15/50\n",
      "150/150 [==============================] - 16s 106ms/step - loss: 195.7467 - val_loss: 198.3068\n",
      "Epoch 16/50\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 1745.9424 - val_loss: 987.6412\n",
      "Epoch 17/50\n",
      "150/150 [==============================] - 16s 105ms/step - loss: 871.7939 - val_loss: 436.7935\n",
      "Epoch 18/50\n",
      "150/150 [==============================] - 15s 100ms/step - loss: 308.4172 - val_loss: 330.1832\n",
      "Epoch 19/50\n",
      "150/150 [==============================] - 15s 98ms/step - loss: 233.6648 - val_loss: 283.6168\n",
      "Epoch 20/50\n",
      "150/150 [==============================] - 16s 105ms/step - loss: 196.0956 - val_loss: 158.4777\n",
      "Epoch 21/50\n",
      "150/150 [==============================] - 15s 103ms/step - loss: 152.7335 - val_loss: 262.2571\n",
      "Epoch 22/50\n",
      "150/150 [==============================] - 16s 105ms/step - loss: 162.5777 - val_loss: 143.8667\n",
      "Epoch 23/50\n",
      "150/150 [==============================] - 15s 100ms/step - loss: 151.3665 - val_loss: 142.6682\n",
      "Epoch 24/50\n",
      "150/150 [==============================] - 15s 99ms/step - loss: 122.7027 - val_loss: 122.8032\n",
      "Epoch 25/50\n",
      "150/150 [==============================] - 17s 115ms/step - loss: 127.6287 - val_loss: 124.2708\n",
      "Epoch 26/50\n",
      "150/150 [==============================] - 15s 97ms/step - loss: 116.7689 - val_loss: 110.9631\n",
      "Epoch 27/50\n",
      "150/150 [==============================] - 14s 96ms/step - loss: 393.1758 - val_loss: 272.2122\n",
      "Epoch 28/50\n",
      "150/150 [==============================] - 15s 100ms/step - loss: 141.1041 - val_loss: 157.5406\n",
      "Epoch 29/50\n",
      "150/150 [==============================] - 15s 103ms/step - loss: 936.1237 - val_loss: 153.2591\n",
      "Epoch 30/50\n",
      "150/150 [==============================] - 16s 104ms/step - loss: 129.2464 - val_loss: 116.9799\n",
      "Epoch 31/50\n",
      "150/150 [==============================] - 16s 103ms/step - loss: 112.5233 - val_loss: 112.0669\n",
      "Epoch 32/50\n",
      "150/150 [==============================] - 15s 99ms/step - loss: 106.5338 - val_loss: 105.2162\n",
      "Epoch 33/50\n",
      "150/150 [==============================] - 14s 94ms/step - loss: 103.6900 - val_loss: 100.6857\n",
      "Epoch 34/50\n",
      "150/150 [==============================] - 14s 96ms/step - loss: 596.5841 - val_loss: 952.8463\n",
      "Epoch 35/50\n",
      "  5/150 [>.............................] - ETA: 9s - loss: 797.8741"
     ]
    }
   ],
   "source": [
    "batch_size = NEvts\n",
    "epochs = 50\n",
    "\n",
    "def mean_abs_theta_vtx(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true[:,0] - y_pred[:,0]))\n",
    "def mean_abs_phi_vtx(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true[:,1] - y_pred[:,1]))\n",
    "def mean_abs_r_vtx(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true[:,2] - y_pred[:,2]))\n",
    "def mean_abs_theta_dir(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true[:,3] - y_pred[:,3]))\n",
    "def mean_abs_phi_dir(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true[:,4] - y_pred[:,4]))\n",
    "\n",
    "def loss_by_R_square(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true[:,0] - y_pred[:,0])+\n",
    "                  K.square(y_true[:,1] - y_pred[:,1])+\n",
    "                  K.square(y_true[:,2] - y_pred[:,2]) )\n",
    "\n",
    "my_network.compile(optimizer=Adam(lr=1e-3), loss='mean_squared_error')#, \n",
    "                                            #loss = loss_by_R_square)\n",
    "                   #metrics=['mean_squared_error', mean_abs_theta_vtx, mean_abs_phi_vtx, mean_abs_r_vtx])#, \n",
    "                                                  #mean_abs_theta_dir, mean_abs_phi_dir])\n",
    "\n",
    "#my_network.load_weights(\"weights_vtx_dir_1el_color_7xLSTM75_8400_400_f950_lr1e-5_N12.h5\")\n",
    "#my_network.load_weights(\"weights_vtx_dir_1el_2xLSTM75_6300_300_f1930_lr1e-5_N9.h5\")\n",
    "\n",
    "\n",
    "#history = my_network.fit_generator(generator=training_generator, epochs=epochs, validation_data=(testX, testY), max_queue_size=10000, workers=10,)# use_multiprocessing=True) \n",
    "\n",
    "history = my_network.fit(trainX, trainY, batch_size=1000, epochs=epochs, verbose=1, validation_data=(testX, testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network.save_weights(\"weights_vtx_dir_1el_color_2xLSTM75_8400_400_f150_lr1e-3_N1.h5\")\n",
    "#my_network.save_weights(\"weights_dir_1el_7xLSTM75_6300_300_f1930_lr1e-5_N3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoY = my_network.predict(testX)\n",
    "\n",
    "print('shape recoY = ', recoY.shape, '   shape testY = ', testY.shape)\n",
    "\n",
    "dY = recoY - testY\n",
    "dY0 = recoY[:,0] - testY[:,0]\n",
    "dY1 = recoY[:,1] - testY[:,1]\n",
    "dY2 = recoY[:,2] - testY[:,2]\n",
    "dR = np.sqrt((recoY[:,0] - testY[:,0])*(recoY[:,0] - testY[:,0]) + (recoY[:,1] - testY[:,1])*(recoY[:,1] - testY[:,1]) + (recoY[:,2] - testY[:,2])*(recoY[:,2] - testY[:,2]) )\n",
    "print(dR.shape, '  dR_mean = ', np.mean(dR))\n",
    "R_true = np.sqrt(testY[:,0]*testY[:,0] + testY[:,1]*testY[:,1] + testY[:,2]*testY[:,2])\n",
    "print(R_true.shape)\n",
    "print(testY.shape)\n",
    "\n",
    "print('dY: ',dY[0:3,:])\n",
    "dRb = dR.reshape(dR.shape[0],1)\n",
    "dY = dY/dRb\n",
    "print('dY_norm: ',dY[0:3,:])\n",
    "#np.random.shuffle(trueDir)\n",
    "vtx_dot_dir = np.sum(dY*trueDir, axis=1)\n",
    "print(vtx_dot_dir.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,8])\n",
    "plt.hist(vtx_dot_dir, bins=60, range=(-1.5,1.5), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'b')\n",
    "plt.title('Correlation between the bias in vertex reconstruction and initial direction of the electron', fontsize=18)\n",
    "plt.xlabel('Dot product between Vtx(bias) and Dir(el)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = np.mean( (recoY.flatten()[:] - testY.flatten()[:])*(recoY.flatten()[:] - testY.flatten()[:]) )\n",
    "print('MSE = ', MSE)\n",
    "\n",
    "#MSE2 = np.mean(np.square(recoY - testY), axis=-1)\n",
    "MSE2 = np.mean(np.square(recoY.flatten()[:] - testY.flatten()[:]), axis=-1)\n",
    "print('MSE2 = ', MSE2)\n",
    "\n",
    "#dTheta_dir = recoY[:,3] - testY[:,3]\n",
    "#dPhi_dir = recoY[:,4] - testY[:,4]\n",
    "print('dY0_Mean_vtx = ' ,np.mean(dY0), '   dY1_Mean_vtx = ', np.mean(dY1), '   dY2_Mean_vtx = ' ,np.mean(dY2))\n",
    "print('dY0_Rms_vtx = ',np.sqrt(np.mean(np.square(dY0))), '    dY1_Rms_vtx = ',np.sqrt(np.mean(np.square(dY1))), '    dY2_Rms_vtx = ',np.sqrt(np.mean(np.square(dY2))))\n",
    "print('resolution_vtx = ', np.sqrt(np.mean(np.square(dY0)) + np.mean(np.square(dY1)) + np.mean(np.square(dY2)) ) )\n",
    "#print('ThetaMean_dir = ',np.mean(dTheta_dir),'   PhiMean_dir = ',np.mean(dPhi_dir))\n",
    "#print('ThetaRms_dir = ',np.sqrt(np.mean(np.square(dTheta_dir))), '   PhiRms_dir = ',np.sqrt(np.mean(np.square(dPhi_dir))))\n",
    "\n",
    "print('reco_Y1_vtx = ',recoY[:,1])\n",
    "print('test_Y1_vtx = ',testY[:,1])\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "\n",
    "#fig_scatt = plt.figure()\n",
    "#plt.scatter(R_true, dR)\n",
    "\n",
    "fig_dYs = plt.figure(figsize=[10,8])\n",
    "plt.hist(dY0, bins=40, range=(-20,20), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'r')\n",
    "plt.hist(dY1, bins=40, range=(-20,20), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'g')\n",
    "plt.hist(dY2, bins=40, range=(-20,20), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'b')\n",
    "plt.title('Vertex Reconstruction',fontsize=18)\n",
    "plt.xlabel('Vtx(reco) - Vtx(true) [cm]',fontsize=16)\n",
    "\n",
    "fig_Y0s = plt.figure()\n",
    "plt.hist(recoY[:,0], bins=50, range=(-500,500), histtype='step',ls='--', alpha = 1.0, lw=3, color= 'r')\n",
    "plt.hist(testY[:,0], bins=50, range=(-500,500), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'r')\n",
    "fig_Y1s = plt.figure()\n",
    "plt.hist(recoY[:,1], bins=50, range=(-500,500), histtype='step',ls='--', alpha = 1.0, lw=3, color= 'g')\n",
    "plt.hist(testY[:,1], bins=50, range=(-500,500), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'g')\n",
    "fig_Y2s = plt.figure()\n",
    "plt.hist(recoY[:,2], bins=50, range=(-500,500), histtype='step',ls='--', alpha = 1.0, lw=3, color= 'b')\n",
    "plt.hist(testY[:,2], bins=50, range=(-500,500), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'b')\n",
    "\n",
    "print('Mean_true_vtx_0 = ', np.mean(testY[:,0]), '   Mean_true_vtx_1 = ' , np.mean(testY[:,1]), 'Mean_true_vtx_2 = ', np.mean(testY[:,2]))\n",
    "print('RMS_true_vtx_0 = ' , np.sqrt(np.mean(np.square(testY[:,0]))), '   RMS_true_vtx_1 = ' , np.sqrt(np.mean(np.square(testY[:,1]))), '  RMS_true_vtx_2 = ', np.sqrt(np.mean(np.square(testY[:,2]))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
