{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex reconstruction in a large Directional Liquid Scintillator Detector\n",
    "Training data location: https://stash.osgconnect.net/public/elagin/vtx_dir_1el_norm/\n",
    "\n",
    "Testing data location: https://stash.osgconnect.net/public/elagin/test_vtx_dir_1el_norm/\n",
    "\n",
    "Data are .npz files with 3 numpy arrays. Where each row correspond to one event.\n",
    "\n",
    "\"x\" is a (1000,6300) array of \"independent variables\", photo-electron hits (theta, phi, and time). Hit coordinate encoding in each row is the folowing: \\\n",
    "theta = x[:,0::3] \\\n",
    "phi = x[:,1::3] \\\n",
    "time = x[:,2::3] \\\n",
    "All data is normalized as theta = theta/pi, phi = phi/(2pi), time = time/maxT (currently maxT=150). \\\n",
    "Each row in \"x\" is padded with -1 to make it a length of 6300 \n",
    "\n",
    "\"y_vtx\" is a (1000,3) array of \"dependent variable\", true x-,y-,z- coordinates of the event vertex to be reconstructed \\\n",
    "\"y_dir\" is a (1000,3) array of \"dependent variable\", true x-,y-,z- coordinates of the unit vector along the original direction of the electron\n",
    "\n",
    "See checking_inputs.ipynb and checking_inputs_norm.ipynb for more details on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape, Lambda, Input, Concatenate, concatenate\n",
    "from keras.layers import Conv1D, Conv2D, Conv2DTranspose, UpSampling2D, MaxPooling1D, MaxPooling2D, LocallyConnected2D, LocallyConnected1D\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Masking\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "maxT = 37\n",
    "maxLambda = 800\n",
    "PAD_VALUE = -1\n",
    "NEvts = 1000\n",
    "pi = 3.141592653589793\n",
    "NFiles = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom generator to load large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class MyGenerator(Sequence):\n",
    "    def __init__(self, x_dir, batch_size):\n",
    "        self.x_dir = x_dir\n",
    "        self.x = os.listdir(x_dir)\n",
    "        self.batch_size = batch_size\n",
    "        print('Generator init complete')\n",
    "        \n",
    "    def __len__(self):\n",
    "        nf = self.batch_size/(NEvts) # calculate number of files\n",
    "        return math.ceil(len(self.x) / nf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_data = np.load(os.path.join(self.x_dir, self.x[idx]))\n",
    "        Y = np.array(batch_data['y_vtx'])\n",
    "        return np.array(batch_data['x']), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator init complete\n"
     ]
    }
   ],
   "source": [
    "training_generator = MyGenerator('/data/Elagin/vtx_dir_1el_color_norm', NEvts)\n",
    "\n",
    "#testing_generator = MyGenerator('/data/Elagin/test_vtx_dir_1el_color_norm', NEvts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data that can fit into RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from x_dir, stop after loading fn_max files\n",
    "# Returns X, Y, Dir\n",
    "# X - independent variables\n",
    "# Y - dependend variables (x-, y-, z- of the vertex in this implementation)\n",
    "# Dir - direction vector (x-, y-, z- componenets), could be made dependent variable\n",
    "def load_test_data_to_memory(x_dir, fn_max=2000) :\n",
    "    X, Y, Dir = np.array([]), np.array([]), np.array([])\n",
    "    f_list = os.listdir(x_dir)\n",
    "    f_list.sort()\n",
    "    i=0\n",
    "    for x in f_list :\n",
    "        batch_data = np.load(os.path.join(x_dir, x))\n",
    "        \n",
    "        X = np.vstack([X, np.array(batch_data['x'])]) if X.size else np.array(batch_data['x'])\n",
    "        Y = np.vstack([Y, np.array(batch_data['y_vtx'])]) if Y.size else np.array(batch_data['y_vtx'])\n",
    "        Dir = np.vstack([Dir, np.array(batch_data['y_dir'])]) if Dir.size else np.array(batch_data['y_dir'])\n",
    "\n",
    "        i+=1\n",
    "        if i%10 == 0:\n",
    "            print(x)\n",
    "            print('i = ', i, '   ', X.shape, '   ', Y.shape, '   ', Dir.shape)\n",
    "        if i>=fn_max:\n",
    "            break\n",
    "    print(X.shape, '   ', Y.shape, '   ', Dir.shape)\n",
    "    return X, Y, Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_batch_18.npz\n",
      "i =  10     (10000, 8400)     (10000, 3)     (10000, 3)\n",
      "f_batch_27.npz\n",
      "i =  20     (20000, 8400)     (20000, 3)     (20000, 3)\n",
      "f_batch_36.npz\n",
      "i =  30     (30000, 8400)     (30000, 3)     (30000, 3)\n",
      "f_batch_45.npz\n",
      "i =  40     (40000, 8400)     (40000, 3)     (40000, 3)\n",
      "(49000, 8400)     (49000, 3)     (49000, 3)\n"
     ]
    }
   ],
   "source": [
    "testX, testY, trueDir = load_test_data_to_memory('/data/Elagin/test_vtx_dir_1el_color_norm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainX, trainY, trueDir_training = load_test_data_to_memory('/data/Elagin/vtx_dir_1el_norm/',200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining various activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_to_pmOne(x) :\n",
    "    return 2*K.sigmoid(x) - 1\n",
    "\n",
    "def mapping_to_pm50(x) :\n",
    "    return mapping_to_pmOne(x)*50\n",
    "\n",
    "def mapping_to_pm300(x) :\n",
    "    return mapping_to_pmOne(x)*300\n",
    "\n",
    "def mapping_to_0_50(x) :\n",
    "    return 50*K.sigmoid(x)\n",
    "\n",
    "\n",
    "def mapping_to_target_range_all( x, target_min=-0.02*pi, target_max=2.02*pi ) :\n",
    "    x02 = K.tanh(x) + 1 # x in range(0,2)\n",
    "    scale = ( target_max-target_min )/2.\n",
    "    return  x02 * scale + target_min\n",
    "\n",
    "def mapping_to_target_range_2pi( x, target_min=-0.00*pi, target_max=2.00*pi ) :\n",
    "    x02 = K.tanh(x) + 1 # x in range(0,2)\n",
    "    scale = ( target_max-target_min )/2.\n",
    "    return  x02 * scale + target_min\n",
    "    #return K.sigmoid(x)*target_max\n",
    "\n",
    "def mapping_to_target_range_pi( x, target_min=-0.00*pi, target_max=1.00*pi ) :\n",
    "    x02 = K.tanh(x) + 1 # x in range(0,2)\n",
    "    scale = ( target_max-target_min )/2.\n",
    "    return  x02 * scale + target_min\n",
    "    #return K.sigmoid(x)*target_max\n",
    "\n",
    "def mapping_to_target_range_phi( x, target_min=0, target_max=2*pi ) :\n",
    "    print('is x keras tensor:', K.is_keras_tensor(x))\n",
    "    print('x shape = ',K.int_shape(x))\n",
    "    print('x shape = ',K.shape(x))\n",
    "    #phi = K.placeholder(shape=(None,1))\n",
    "    phi = Input([1])\n",
    "    #theta = K.placeholder(shape=(None,1))\n",
    "    theta = Input([1])\n",
    "    #r = K.placeholder(shape=(None,1))\n",
    "    r = Input([1])\n",
    "    print('phi shape = ',K.int_shape(phi))\n",
    "    print('is phi keras tensor:', K.is_keras_tensor(phi))\n",
    "    phi = K.concatenate([phi, [K.tanh(x[:,1]) + 1]], axis=0) # x in range(0,2)\n",
    "    #phi = K.tanh(x[:,1]) + 1\n",
    "    theta = K.concatenate([theta, [x[:,0]]], axis=0)\n",
    "    #theta = x[:,0]\n",
    "    r = K.concatenate([r, [x[:,2]]], axis=0)\n",
    "    #r = x[:,2]\n",
    "    print('phi shape = ',K.int_shape(phi))\n",
    "    print('is phi keras tensor:', K.is_keras_tensor(phi))\n",
    "    print('theta shape = ',K.int_shape(theta))\n",
    "    print('r shape = ',K.int_shape(r))\n",
    "    scale = ( target_max-target_min )/2.\n",
    "    phi =  phi * scale + target_min\n",
    "    #phi = K.cast(phi, dtype='int32')\n",
    "    #phi = K.reshape(phi, (,1))\n",
    "    print('phi shape = ',K.int_shape(phi))\n",
    "    \n",
    "    res = K.concatenate([theta, phi, r], axis=1)\n",
    "    print('res shape = ',K.int_shape(res))\n",
    "    print('res shape = ',K.shape(res))\n",
    "    print('is res keras tensor:', K.is_keras_tensor(res))\n",
    "    x[:,1].assign(phi)\n",
    "    return x\n",
    "\n",
    "def mapping_to_target_range_phi2( x, target_min=0, target_max=2*pi ) :\n",
    "    scale = ( target_max-target_min )/2.\n",
    "    phi = K.placeholder(shape=(None,1))\n",
    "    #phi = Input([1])\n",
    "    #phi = (K.tanh(x[:,1]) + 1)*scale + target_min\n",
    "    phi = K.concatenate([phi, [(K.tanh(x[:,1]) + 1)*scale + target_min]], axis=0)\n",
    "    x[:,1].assign(phi)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    N=8400\n",
    "    lstm_seq = 400\n",
    "    lstm_steps = int(N/lstm_seq)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Reshape(input_shape=(N,), target_shape=(lstm_steps,lstm_seq)))\n",
    "    \n",
    "    model.add(Masking(mask_value=-1, input_shape=(lstm_steps, lstm_seq)))\n",
    "    \n",
    "    model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "    model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "    model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "    model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "    model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "    model.add(LSTM(75, return_sequences=True, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "    model.add(LSTM(75, input_shape=(lstm_steps,lstm_seq), dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(3, activation=mapping_to_pm300))\n",
    "#    model.add(Dense(3, activation=None))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network=createModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 21, 400)           0         \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 21, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 21, 75)            142800    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 21, 75)            45300     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 21, 75)            45300     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 21, 75)            45300     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 21, 75)            45300     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 21, 75)            45300     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 75)                45300     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               19456     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 500,619\n",
      "Trainable params: 500,619\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-06f2fc6eeac8>:29: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/100\n",
      "942/950 [============================>.] - ETA: 2s - loss: 49.6859"
     ]
    }
   ],
   "source": [
    "batch_size = NEvts\n",
    "epochs = 100\n",
    "\n",
    "def mean_abs_theta_vtx(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true[:,0] - y_pred[:,0]))\n",
    "def mean_abs_phi_vtx(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true[:,1] - y_pred[:,1]))\n",
    "def mean_abs_r_vtx(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true[:,2] - y_pred[:,2]))\n",
    "def mean_abs_theta_dir(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true[:,3] - y_pred[:,3]))\n",
    "def mean_abs_phi_dir(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true[:,4] - y_pred[:,4]))\n",
    "\n",
    "def loss_by_R_square(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true[:,0] - y_pred[:,0])+\n",
    "                  K.square(y_true[:,1] - y_pred[:,1])+\n",
    "                  K.square(y_true[:,2] - y_pred[:,2]) )\n",
    "\n",
    "my_network.compile(optimizer=Adam(lr=1e-5), loss='mean_squared_error')#, \n",
    "                                            #loss = loss_by_R_square)\n",
    "                   #metrics=['mean_squared_error', mean_abs_theta_vtx, mean_abs_phi_vtx, mean_abs_r_vtx])#, \n",
    "                                                  #mean_abs_theta_dir, mean_abs_phi_dir])\n",
    "\n",
    "my_network.load_weights(\"weights_vtx_dir_1el_color_7xLSTM75_8400_400_f950_lr1e-5_N5.h5\")\n",
    "#my_network.load_weights(\"weights_vtx_dir_1el_2xLSTM75_6300_300_f1930_lr1e-5_N9.h5\")\n",
    "\n",
    "\n",
    "history = my_network.fit_generator(generator=training_generator, epochs=epochs, validation_data=(testX, testY)) \n",
    "\n",
    "#history = my_network.fit(trainX, trainY, batch_size=1000, epochs=epochs, verbose=1, validation_data=(testX, testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network.save_weights(\"weights_vtx_dir_1el_color_7xLSTM75_8400_400_f950_lr1e-5_N6.h5\")\n",
    "#my_network.save_weights(\"weights_dir_1el_7xLSTM75_6300_300_f1930_lr1e-5_N3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoY = my_network.predict(testX)\n",
    "\n",
    "print('shape recoY = ', recoY.shape, '   shape testY = ', testY.shape)\n",
    "\n",
    "dY = recoY - testY\n",
    "dY0 = recoY[:,0] - testY[:,0]\n",
    "dY1 = recoY[:,1] - testY[:,1]\n",
    "dY2 = recoY[:,2] - testY[:,2]\n",
    "dR = np.sqrt((recoY[:,0] - testY[:,0])*(recoY[:,0] - testY[:,0]) + (recoY[:,1] - testY[:,1])*(recoY[:,1] - testY[:,1]) + (recoY[:,2] - testY[:,2])*(recoY[:,2] - testY[:,2]) )\n",
    "print(dR.shape, '  dR_mean = ', np.mean(dR))\n",
    "R_true = np.sqrt(testY[:,0]*testY[:,0] + testY[:,1]*testY[:,1] + testY[:,2]*testY[:,2])\n",
    "print(R_true.shape)\n",
    "print(testY.shape)\n",
    "\n",
    "print('dY: ',dY[0:3,:])\n",
    "dRb = dR.reshape(dR.shape[0],1)\n",
    "dY = dY/dRb\n",
    "print('dY_norm: ',dY[0:3,:])\n",
    "#np.random.shuffle(trueDir)\n",
    "vtx_dot_dir = np.sum(dY*trueDir, axis=1)\n",
    "print(vtx_dot_dir.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,8])\n",
    "plt.hist(vtx_dot_dir, bins=60, range=(-1.5,1.5), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'b')\n",
    "plt.title('Correlation between the bias in vertex reconstruction and initial direction of the electron', fontsize=18)\n",
    "plt.xlabel('Dot product between Vtx(bias) and Dir(el)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = np.mean( (recoY.flatten()[:] - testY.flatten()[:])*(recoY.flatten()[:] - testY.flatten()[:]) )\n",
    "print('MSE = ', MSE)\n",
    "\n",
    "#MSE2 = np.mean(np.square(recoY - testY), axis=-1)\n",
    "MSE2 = np.mean(np.square(recoY.flatten()[:] - testY.flatten()[:]), axis=-1)\n",
    "print('MSE2 = ', MSE2)\n",
    "\n",
    "#dTheta_dir = recoY[:,3] - testY[:,3]\n",
    "#dPhi_dir = recoY[:,4] - testY[:,4]\n",
    "print('dY0_Mean_vtx = ' ,np.mean(dY0), '   dY1_Mean_vtx = ', np.mean(dY1), '   dY2_Mean_vtx = ' ,np.mean(dY2))\n",
    "print('dY0_Rms_vtx = ',np.sqrt(np.mean(np.square(dY0))), '    dY1_Rms_vtx = ',np.sqrt(np.mean(np.square(dY1))), '    dY2_Rms_vtx = ',np.sqrt(np.mean(np.square(dY2))))\n",
    "print('resolution_vtx = ', np.sqrt(np.mean(np.square(dY0)) + np.mean(np.square(dY1)) + np.mean(np.square(dY2)) ) )\n",
    "#print('ThetaMean_dir = ',np.mean(dTheta_dir),'   PhiMean_dir = ',np.mean(dPhi_dir))\n",
    "#print('ThetaRms_dir = ',np.sqrt(np.mean(np.square(dTheta_dir))), '   PhiRms_dir = ',np.sqrt(np.mean(np.square(dPhi_dir))))\n",
    "\n",
    "print('reco_Y1_vtx = ',recoY[:,1])\n",
    "print('test_Y1_vtx = ',testY[:,1])\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "\n",
    "#fig_scatt = plt.figure()\n",
    "#plt.scatter(R_true, dR)\n",
    "\n",
    "fig_dYs = plt.figure(figsize=[10,8])\n",
    "plt.hist(dY0, bins=40, range=(-20,20), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'r')\n",
    "plt.hist(dY1, bins=40, range=(-20,20), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'g')\n",
    "plt.hist(dY2, bins=40, range=(-20,20), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'b')\n",
    "plt.title('Vertex Reconstruction',fontsize=18)\n",
    "plt.xlabel('Vtx(reco) - Vtx(true) [cm]',fontsize=16)\n",
    "\n",
    "fig_Y0s = plt.figure()\n",
    "plt.hist(recoY[:,0], bins=50, range=(-500,500), histtype='step',ls='--', alpha = 1.0, lw=3, color= 'r')\n",
    "plt.hist(testY[:,0], bins=50, range=(-500,500), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'r')\n",
    "fig_Y1s = plt.figure()\n",
    "plt.hist(recoY[:,1], bins=50, range=(-500,500), histtype='step',ls='--', alpha = 1.0, lw=3, color= 'g')\n",
    "plt.hist(testY[:,1], bins=50, range=(-500,500), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'g')\n",
    "fig_Y2s = plt.figure()\n",
    "plt.hist(recoY[:,2], bins=50, range=(-500,500), histtype='step',ls='--', alpha = 1.0, lw=3, color= 'b')\n",
    "plt.hist(testY[:,2], bins=50, range=(-500,500), histtype='step',ls='solid', alpha = 1.0, lw=3, color= 'b')\n",
    "\n",
    "print('Mean_true_vtx_0 = ', np.mean(testY[:,0]), '   Mean_true_vtx_1 = ' , np.mean(testY[:,1]), 'Mean_true_vtx_2 = ', np.mean(testY[:,2]))\n",
    "print('RMS_true_vtx_0 = ' , np.sqrt(np.mean(np.square(testY[:,0]))), '   RMS_true_vtx_1 = ' , np.sqrt(np.mean(np.square(testY[:,1]))), '  RMS_true_vtx_2 = ', np.sqrt(np.mean(np.square(testY[:,2]))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
